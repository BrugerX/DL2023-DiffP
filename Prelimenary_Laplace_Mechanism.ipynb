{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Steps in this notebook:\n",
    "\n",
    "#### Step 1: Load the model and the image\n",
    "\n",
    "### Step 2: Create the Laplace mechanism function by modifying the original ML model's way of encoding the image\n",
    "\n",
    "### Step 3: Plot different images (Original vs Encoded vs Encoded w. Noise) for CelebAHQ and PublicAHQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image,ImageShow\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from taming.models.cond_transformer import Net2NetTransformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the dependencies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\driptoohard\\pycharmprojects\\taming-transformers\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\driptoohard\\pycharmprojects\\taming-transformers\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\driptoohard\\pycharmprojects\\taming-transformers\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\driptoohard\\pycharmprojects\\taming-transformers\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\driptoohard\\pycharmprojects\\taming-transformers\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\driptoohard\\pycharmprojects\\taming-transformers\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops transformers\n",
    "\n",
    "sys.path.append(\".\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare configurations of the CelebAHQ model as well as checkpoints."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  params:\n",
      "    batch_size: 24\n",
      "    num_workers: 24\n",
      "    train:\n",
      "      params:\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.CelebAHQTrain\n",
      "    validation:\n",
      "      params:\n",
      "        size: 256\n",
      "      target: taming.data.faceshq.CelebAHQValidation\n",
      "  target: cutlit.DataModuleFromConfig\n",
      "model:\n",
      "  base_learning_rate: 0.0625\n",
      "  params:\n",
      "    cond_stage_config: __is_unconditional__\n",
      "    first_stage_config:\n",
      "      params:\n",
      "        ddconfig:\n",
      "          attn_resolutions:\n",
      "          - 16\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 1\n",
      "          - 2\n",
      "          - 2\n",
      "          - 4\n",
      "          double_z: false\n",
      "          dropout: 0.0\n",
      "          in_channels: 3\n",
      "          num_res_blocks: 2\n",
      "          out_ch: 3\n",
      "          resolution: 256\n",
      "          z_channels: 256\n",
      "        embed_dim: 256\n",
      "        lossconfig:\n",
      "          target: taming.modules.losses.vqperceptual.DummyLoss\n",
      "        n_embed: 1024\n",
      "      target: taming.models.vqgan.VQModel\n",
      "    first_stage_key: image\n",
      "    transformer_config:\n",
      "      params:\n",
      "        block_size: 256\n",
      "        n_embd: 1664\n",
      "        n_head: 16\n",
      "        n_layer: 24\n",
      "        vocab_size: 1024\n",
      "      target: taming.modules.transformer.mingpt.GPT\n",
      "  target: taming.models.cond_transformer.Net2NetTransformer\n",
      "\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "Using no cond stage. Assuming the training is intended to be unconditional. Prepending 0 as a sos token.\n"
     ]
    }
   ],
   "source": [
    "#Prepare CelebAHQ configurations\n",
    "config_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers\\scripts\\2021-04-23T18-11-19-project.yaml\"\n",
    "celebAHQ_config = OmegaConf.load(config_path)\n",
    "print(yaml.dump(OmegaConf.to_container(celebAHQ_config)))\n",
    "\n",
    "#Init model with the chosen architecture and configurations\n",
    "model = Net2NetTransformer(**celebAHQ_config.model.params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Load checkpoints\n",
    "ckpt_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers\\scripts\\taming-transformers\\configs\\last.ckpt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "model.load_state_dict(sd)\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Put model in evaluation mode\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load example data\n",
    "\n",
    "Load an example segmentation and visualize."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the function to load an image as a tensor\n",
    "def load_image_tensor(path):\n",
    "    image = Image.open(path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.PILToTensor()\n",
    "    ])\n",
    "    img_tensor = transform(image).permute(1,2,0).float()/255\n",
    "    return img_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "# Load the images\n",
    "AHQ_example_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers\\scripts\\taming-transformers\\data\\ffhq_images\\01000\\01000.jpg\"\n",
    "FFHQ_example_path = \"C:/Users/DripTooHard/PycharmProjects/taming-transformers/scripts/taming-transformers/FFHQ_image.png\"\n",
    "\n",
    "AHQ_image = load_image_tensor(AHQ_example_path)\n",
    "FFHQ_image = load_image_tensor(FFHQ_example_path)\n",
    "\n",
    "# Plot the images with titles\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(AHQ_image[0].permute(1, 2, 0))\n",
    "plt.title('CelebAHQ')\n",
    "plt.axis('off')  # Hide the axis\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(FFHQ_image[0].permute(1, 2, 0))\n",
    "plt.title('FFHQ')\n",
    "plt.axis('off')  # Hide the axis\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2: We now add the possibility of encoding with the Laplace mechanism added before the quantization step.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.encode_to_z(AHQ_image,2)[0].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Step 3:\n",
    "\n",
    "We now recreate the images with noise and plot them next to eachother.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "def deconstruct_reconstruct(image, epsilon):\n",
    "    image = image.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image_zq, image_z_indices = model.encode_to_z(image, epsilon)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "#Ignore this for now\n",
    "def deconstruct_reconstruct_mix(image1,image2,epsilon):\n",
    "    image1 = image1.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image2 = image2.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image_zq, image_z_indices = model.encode_to_z_mix(image1,image2, epsilon)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "\n",
    "step_size = 0.01\n",
    "max_range = 1\n",
    "\n",
    "epsilon_values = np.arange(0,max_range,step_size)\n",
    "\n",
    "folder = fr\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers\\scripts\\Noisy Reconstructions\\Laplace{max_range}{step_size}\"\n",
    "#os.mkdir(folder)\n",
    "\n",
    "\n",
    "if(True):\n",
    "    for epsilon in epsilon_values:\n",
    "        # Assuming AHQ_image and FFHQ_image are defined and are tensors\n",
    "        AHQ_recon = deconstruct_reconstruct(AHQ_image, epsilon)\n",
    "        FFHQ_recon = deconstruct_reconstruct(FFHQ_image, epsilon)\n",
    "\n",
    "        # Plot the images with titles\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(AHQ_recon[0].permute(1, 2, 0))\n",
    "        plt.title('CelebAHQ-Reconstructed')\n",
    "        plt.axis('off')  # Hide the axis\n",
    "\n",
    "        # Use numpy to clip the values and ensure the correct data type for imsave\n",
    "\n",
    "        plt.imsave(f\"{folder}\\AHQ\\{epsilon}CelebAHQ-Reconstructed.png\", AHQ_recon[0].permute(1, 2, 0).numpy().clip(0, 1))\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(FFHQ_recon[0].permute(1, 2, 0))\n",
    "        plt.title('FFHQ-reconstructed')\n",
    "        plt.axis('off')  # Hide the axis\n",
    "\n",
    "        # Use numpy to clip the values and ensure the correct data type for imsave\n",
    "        plt.imsave(f\"{folder}\\FFHQ\\{epsilon}FFHQ-reconstructed.png\", FFHQ_recon[0].permute(1, 2, 0).numpy().clip(0, 1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.dot([1,1,1,1],[1,1,1,1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}