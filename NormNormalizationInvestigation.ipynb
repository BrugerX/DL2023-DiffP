{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image,ImageShow\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "import seaborn as sb\n",
    "import torchvision.transforms as transforms\n",
    "from taming.data.faceshq import NumpyPaths\n",
    "\n",
    "\n",
    "from taming.models.cond_transformer import Net2NetTransformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the dependencies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install omegaconf>=2.0.0 pytorch-lightning>=1.0.8 einops transformers\n",
    "\n",
    "sys.path.append(\".\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prepare configurations of the CelebAHQ model as well as checkpoints."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Prepare CelebAHQ configurations\n",
    "config_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers\\scripts\\taming-transformers\\configs\\2021-04-23T18-11-19-project.yaml\"\n",
    "celebAHQ_config = OmegaConf.load(config_path)\n",
    "print(yaml.dump(OmegaConf.to_container(celebAHQ_config)))\n",
    "\n",
    "#Init model with the chosen architecture and configurations\n",
    "model = Net2NetTransformer(**celebAHQ_config.model.params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Load checkpoints\n",
    "ckpt_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers\\scripts\\taming-transformers\\configs\\CelebAHQ.ckpt\"\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "model.load_state_dict(sd)\n",
    "missing, unexpected = model.load_state_dict(sd, strict=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Put model in evaluation mode\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Investigating the norms of the dict\n",
    "\n",
    "We will now iterate through the indices of the codebook dictionary and investigate what the"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "norms = []\n",
    "\n",
    "for params in model.first_stage_model.quantize.embedding.parameters():\n",
    "    for z in params:\n",
    "        norms += [np.linalg.norm(z)]\n",
    "\n",
    "# Creating a histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sb.histplot(data=norms, bins=30, stat=\"probability\")\n",
    "plt.xlabel(\"Norm Lengths\")\n",
    "plt.title(\"Codebook Norm Lengths\")\n",
    "\n",
    "# Calculating the percentage of norms above the length of 1\n",
    "norms_above_1 = [norm for norm in norms if norm > 1]\n",
    "percentage_above_1 = len(norms_above_1) / len(norms) * 100\n",
    "print(f\"Percentage of norms above the length of 1: {percentage_above_1}%\")\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TODO: Add some examples of what the embeddings close to 0 and those far away show.\n",
    "for params in model.first_stage_model.quantize.embedding.parameters():\n",
    "    print(params.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: We will now look at the in practice norm distribution\n",
    "\n",
    "As the number of embeddings that are initialized is static and does not depend on the actual number of codes needed to meaningfully represent our dataset, we would like to know; Which code lengths are actually used.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "import albumentations\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_image_paths(base_path):\n",
    "    image_paths = []\n",
    "    for file_name in os.listdir(base_path):\n",
    "        if file_name.endswith('.png'):\n",
    "            full_path = os.path.join(base_path, file_name)\n",
    "            image_paths.append(full_path)\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def display_numpy_array_as_image(numpy_array):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    numpy_array (numpy.ndarray): A Numpy array representing an image.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Ensure the numpy array is in the right format (uint8)\n",
    "    if numpy_array.dtype != np.uint8:\n",
    "        numpy_array = (numpy_array * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert the Numpy array to a PIL image\n",
    "    image = Image.fromarray(numpy_array)\n",
    "\n",
    "    # Display the image\n",
    "    image.show()\n",
    "\n",
    "\n",
    "class ImagePaths(Dataset):\n",
    "    def __init__(self, paths, size=None, random_crop=False, labels=None):\n",
    "        self.size = size\n",
    "        self.random_crop = random_crop\n",
    "\n",
    "        self.labels = dict() if labels is None else labels\n",
    "        self.labels[\"file_path_\"] = paths\n",
    "        self._length = len(paths)\n",
    "\n",
    "        if self.size is not None and self.size > 0:\n",
    "            self.rescaler = albumentations.SmallestMaxSize(max_size = self.size)\n",
    "            if not self.random_crop:\n",
    "                self.cropper = albumentations.CenterCrop(height=self.size,width=self.size)\n",
    "            else:\n",
    "                self.cropper = albumentations.RandomCrop(height=self.size,width=self.size)\n",
    "            self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n",
    "        else:\n",
    "            self.preprocessor = lambda **kwargs: kwargs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        if not image.mode == \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = self.preprocessor(image=image)[\"image\"]\n",
    "        image = (image/127.5 - 1.0).astype(np.float32)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        example = dict()\n",
    "        example[\"image\"] = self.preprocess_image(self.labels[\"file_path_\"][i])\n",
    "        for k in self.labels:\n",
    "            example[k] = self.labels[k][i]\n",
    "        return example\n",
    "\n",
    "\n",
    "class NumpyPaths(ImagePaths):\n",
    "    def preprocess_image(self, image_path):\n",
    "        image = self.load_image_nparray(image_path) # 3 x 1024 x 1024\n",
    "        image = Image.fromarray(image, mode=\"RGB\")\n",
    "        image = np.array(image).astype(np.uint8)\n",
    "        image = self.preprocessor(image=image)[\"image\"]\n",
    "        image = (image/127.5 - 1.0).astype(np.float32)\n",
    "        return image\n",
    "\n",
    "\n",
    "    def path_to_tensor(self, path):\n",
    "        preprocessed_image_np = self.preprocess_image(path)\n",
    "        image_tensor = torch.from_numpy(preprocessed_image_np).float()\n",
    "        image_tensor = image_tensor.permute(2, 0, 1)  # Permute to CxHxW\n",
    "        return image_tensor.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "\n",
    "\n",
    "    def load_image_nparray(self,path):\n",
    "        image = Image.open(path)\n",
    "        return np.array(image)\n",
    "\n",
    "base_image_path = r\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers\\scripts\\taming-transformers\\data\\ffhq_images\\01000\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "image_size = 1024\n",
    "\n",
    "FFHQ_image_paths = get_image_paths(base_image_path)\n",
    "prep = NumpyPaths(FFHQ_image_paths,size = image_size, random_crop=False)\n",
    "test_image = prep.__getitem__(0)[\"image\"]\n",
    "\n",
    "#Check that the images look alright\n",
    "display_numpy_array_as_image(test_image[0].permute((1,2,0)).cpu().detach().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[133], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m image \u001B[38;5;241m=\u001B[39m image\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mFloatTensor)  \u001B[38;5;66;03m# Ensure the image tensor is of type FloatTensor\u001B[39;00m\n\u001B[0;32m     14\u001B[0m image_zq, image_z_indices \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mencode_to_z(image, epsilon)\n\u001B[1;32m---> 15\u001B[0m \u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Tensor' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "\n",
    "def deconstruct_reconstruct(image, epsilon):\n",
    "    image = image.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image_zq, image_z_indices = model.encode_to_z(image, epsilon)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "def deconstruct_reconstruct_mix(image1,image2,epsilon):\n",
    "    image1 = image1.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image2 = image2.type(torch.FloatTensor)  # Ensure the image tensor is of type FloatTensor\n",
    "    image_zq, image_z_indices = model.encode_to_z_mix(image1,image2, epsilon)\n",
    "    image_recon = model.decode_to_img(image_z_indices, image_zq.shape)\n",
    "    return image_recon\n",
    "\n",
    "\n",
    "step_size = 0.01\n",
    "max_range = 1\n",
    "\n",
    "epsilon_values = np.arange(0,max_range,step_size)\n",
    "\n",
    "folder = fr\"C:\\Users\\DripTooHard\\PycharmProjects\\taming-transformers\\scripts\\Noisy Reconstructions\\Laplace{max_range}{step_size}\"\n",
    "#os.mkdir(folder)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}